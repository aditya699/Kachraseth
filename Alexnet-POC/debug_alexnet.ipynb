{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is a testing notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the library\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms,datasets\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Data Transformations\n",
    "transform=transforms.Compose([\n",
    "    transforms.Resize((227,227)),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the training dataset\n",
    "train_dataset=datasets.ImageFolder(root='../Lenet-5-POC/Data/train',transform=transform)\n",
    "\n",
    "#Define the validation dataset\n",
    "val_dataset=datasets.ImageFolder(root='../Lenet-5-POC/Data/val',transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader for training\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Create DataLoader for validation\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Alexnet(num_classes):\n",
    "    model=nn.Sequential(\n",
    "        #First Convolution Layer\n",
    "        nn.Conv2d(in_channels=3,out_channels=96,kernel_size=11,stride=4,padding=2),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.MaxPool2d(kernel_size=3,stride=2),\n",
    "\n",
    "        # Second Convolutional Layer\n",
    "        nn.Conv2d(96, 256, kernel_size=5, padding=2),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        \n",
    "        # Third Convolutional Layer\n",
    "        nn.Conv2d(256, 384, kernel_size=3, padding=1),\n",
    "        nn.ReLU(inplace=True),\n",
    "        \n",
    "        # Fourth Convolutional Layer\n",
    "        nn.Conv2d(384, 384, kernel_size=3, padding=1),\n",
    "        nn.ReLU(inplace=True),\n",
    "        \n",
    "        # Fifth Convolutional Layer\n",
    "        nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        \n",
    "        # Adaptive Average Pooling Layer\n",
    "        nn.AdaptiveAvgPool2d((6, 6)),\n",
    "        \n",
    "        # Flatten Layer\n",
    "        nn.Flatten(),\n",
    "        \n",
    "        # First Fully Connected Layer\n",
    "        nn.Linear(256 * 6 * 6, 4096),\n",
    "        nn.ReLU(inplace=True),\n",
    "        \n",
    "        # Second Fully Connected Layer\n",
    "        nn.Linear(4096, 4096),\n",
    "        nn.ReLU(inplace=True),\n",
    "        \n",
    "        # Output Layer\n",
    "        nn.Linear(4096, num_classes)  # Assuming binary classification, adjust num_classes accordingly\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Loop\n",
    "def train(model,train_loader,criterion,optimizer,num_epochs=1):\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_idx,(data, target) in enumerate(train_loader):\n",
    "            # Clear the gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(data)\n",
    "            \n",
    "            # Compute the loss\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update the weights\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Print training progress\n",
    "            if batch_idx % 5 == 0:\n",
    "                \n",
    "                print(f'Epoch [{epoch + 1}/{num_epochs}], Batch [{batch_idx}/{len(train_loader)}], Loss: {loss.item():.4f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Batch [0/706], Loss: 0.6935\n",
      "\n",
      "Epoch [1/100], Batch [5/706], Loss: 0.6914\n",
      "\n",
      "Epoch [1/100], Batch [10/706], Loss: 0.6927\n",
      "\n",
      "Epoch [1/100], Batch [15/706], Loss: 0.6937\n",
      "\n",
      "Epoch [1/100], Batch [20/706], Loss: 0.6926\n",
      "\n",
      "Epoch [1/100], Batch [25/706], Loss: 0.6921\n",
      "\n",
      "Epoch [1/100], Batch [30/706], Loss: 0.6930\n",
      "\n",
      "Epoch [1/100], Batch [35/706], Loss: 0.6937\n",
      "\n",
      "Epoch [1/100], Batch [40/706], Loss: 0.6928\n",
      "\n",
      "Epoch [1/100], Batch [45/706], Loss: 0.6922\n",
      "\n",
      "Epoch [1/100], Batch [50/706], Loss: 0.6934\n",
      "\n",
      "Epoch [1/100], Batch [55/706], Loss: 0.6921\n",
      "\n",
      "Epoch [1/100], Batch [60/706], Loss: 0.6930\n",
      "\n",
      "Epoch [1/100], Batch [65/706], Loss: 0.6927\n",
      "\n",
      "Epoch [1/100], Batch [70/706], Loss: 0.6933\n",
      "\n",
      "Epoch [1/100], Batch [75/706], Loss: 0.6930\n",
      "\n",
      "Epoch [1/100], Batch [80/706], Loss: 0.6923\n",
      "\n",
      "Epoch [1/100], Batch [85/706], Loss: 0.6928\n",
      "\n",
      "Epoch [1/100], Batch [90/706], Loss: 0.6938\n",
      "\n",
      "Epoch [1/100], Batch [95/706], Loss: 0.6930\n",
      "\n",
      "Epoch [1/100], Batch [100/706], Loss: 0.6926\n",
      "\n",
      "Epoch [1/100], Batch [105/706], Loss: 0.6923\n",
      "\n",
      "Epoch [1/100], Batch [110/706], Loss: 0.6939\n",
      "\n",
      "Epoch [1/100], Batch [115/706], Loss: 0.6926\n",
      "\n",
      "Epoch [1/100], Batch [120/706], Loss: 0.6933\n",
      "\n",
      "Epoch [1/100], Batch [125/706], Loss: 0.6919\n",
      "\n",
      "Epoch [1/100], Batch [130/706], Loss: 0.6926\n",
      "\n",
      "Epoch [1/100], Batch [135/706], Loss: 0.6935\n",
      "\n",
      "Epoch [1/100], Batch [140/706], Loss: 0.6922\n",
      "\n",
      "Epoch [1/100], Batch [145/706], Loss: 0.6938\n",
      "\n",
      "Epoch [1/100], Batch [150/706], Loss: 0.6928\n",
      "\n",
      "Epoch [1/100], Batch [155/706], Loss: 0.6924\n",
      "\n",
      "Epoch [1/100], Batch [160/706], Loss: 0.6926\n",
      "\n",
      "Epoch [1/100], Batch [165/706], Loss: 0.6928\n",
      "\n",
      "Epoch [1/100], Batch [170/706], Loss: 0.6926\n",
      "\n",
      "Epoch [1/100], Batch [175/706], Loss: 0.6921\n",
      "\n",
      "Epoch [1/100], Batch [180/706], Loss: 0.6927\n",
      "\n",
      "Epoch [1/100], Batch [185/706], Loss: 0.6928\n",
      "\n",
      "Epoch [1/100], Batch [190/706], Loss: 0.6926\n",
      "\n",
      "Epoch [1/100], Batch [195/706], Loss: 0.6926\n",
      "\n",
      "Epoch [1/100], Batch [200/706], Loss: 0.6931\n",
      "\n",
      "Epoch [1/100], Batch [205/706], Loss: 0.6932\n",
      "\n",
      "Epoch [1/100], Batch [210/706], Loss: 0.6937\n",
      "\n",
      "Epoch [1/100], Batch [215/706], Loss: 0.6933\n",
      "\n",
      "Epoch [1/100], Batch [220/706], Loss: 0.6932\n",
      "\n",
      "Epoch [1/100], Batch [225/706], Loss: 0.6933\n",
      "\n",
      "Epoch [1/100], Batch [230/706], Loss: 0.6933\n",
      "\n",
      "Epoch [1/100], Batch [235/706], Loss: 0.6935\n",
      "\n",
      "Epoch [1/100], Batch [240/706], Loss: 0.6921\n",
      "\n",
      "Epoch [1/100], Batch [245/706], Loss: 0.6926\n",
      "\n",
      "Epoch [1/100], Batch [250/706], Loss: 0.6931\n",
      "\n",
      "Epoch [1/100], Batch [255/706], Loss: 0.6933\n",
      "\n",
      "Epoch [1/100], Batch [260/706], Loss: 0.6934\n",
      "\n",
      "Epoch [1/100], Batch [265/706], Loss: 0.6928\n",
      "\n",
      "Epoch [1/100], Batch [270/706], Loss: 0.6933\n",
      "\n",
      "Epoch [1/100], Batch [275/706], Loss: 0.6930\n",
      "\n",
      "Epoch [1/100], Batch [280/706], Loss: 0.6929\n",
      "\n",
      "Epoch [1/100], Batch [285/706], Loss: 0.6931\n",
      "\n",
      "Epoch [1/100], Batch [290/706], Loss: 0.6923\n",
      "\n",
      "Epoch [1/100], Batch [295/706], Loss: 0.6929\n",
      "\n",
      "Epoch [1/100], Batch [300/706], Loss: 0.6937\n",
      "\n",
      "Epoch [1/100], Batch [305/706], Loss: 0.6939\n",
      "\n",
      "Epoch [1/100], Batch [310/706], Loss: 0.6923\n",
      "\n",
      "Epoch [1/100], Batch [315/706], Loss: 0.6932\n",
      "\n",
      "Epoch [1/100], Batch [320/706], Loss: 0.6933\n",
      "\n",
      "Epoch [1/100], Batch [325/706], Loss: 0.6929\n",
      "\n",
      "Epoch [1/100], Batch [330/706], Loss: 0.6928\n",
      "\n",
      "Epoch [1/100], Batch [335/706], Loss: 0.6930\n",
      "\n",
      "Epoch [1/100], Batch [340/706], Loss: 0.6926\n",
      "\n",
      "Epoch [1/100], Batch [345/706], Loss: 0.6931\n",
      "\n",
      "Epoch [1/100], Batch [350/706], Loss: 0.6932\n",
      "\n",
      "Epoch [1/100], Batch [355/706], Loss: 0.6929\n",
      "\n",
      "Epoch [1/100], Batch [360/706], Loss: 0.6923\n",
      "\n",
      "Epoch [1/100], Batch [365/706], Loss: 0.6928\n",
      "\n",
      "Epoch [1/100], Batch [370/706], Loss: 0.6921\n",
      "\n",
      "Epoch [1/100], Batch [375/706], Loss: 0.6933\n",
      "\n",
      "Epoch [1/100], Batch [380/706], Loss: 0.6937\n",
      "\n",
      "Epoch [1/100], Batch [385/706], Loss: 0.6937\n",
      "\n",
      "Epoch [1/100], Batch [390/706], Loss: 0.6925\n",
      "\n",
      "Epoch [1/100], Batch [395/706], Loss: 0.6928\n",
      "\n",
      "Epoch [1/100], Batch [400/706], Loss: 0.6930\n",
      "\n",
      "Epoch [1/100], Batch [405/706], Loss: 0.6930\n",
      "\n",
      "Epoch [1/100], Batch [410/706], Loss: 0.6928\n",
      "\n",
      "Epoch [1/100], Batch [415/706], Loss: 0.6928\n",
      "\n",
      "Epoch [1/100], Batch [420/706], Loss: 0.6931\n",
      "\n",
      "Epoch [1/100], Batch [425/706], Loss: 0.6930\n",
      "\n",
      "Epoch [1/100], Batch [430/706], Loss: 0.6926\n",
      "\n",
      "Epoch [1/100], Batch [435/706], Loss: 0.6931\n",
      "\n",
      "Epoch [1/100], Batch [440/706], Loss: 0.6921\n",
      "\n",
      "Epoch [1/100], Batch [445/706], Loss: 0.6940\n",
      "\n",
      "Epoch [1/100], Batch [450/706], Loss: 0.6931\n",
      "\n",
      "Epoch [1/100], Batch [455/706], Loss: 0.6930\n",
      "\n",
      "Epoch [1/100], Batch [460/706], Loss: 0.6926\n",
      "\n",
      "Epoch [1/100], Batch [465/706], Loss: 0.6924\n",
      "\n",
      "Epoch [1/100], Batch [470/706], Loss: 0.6930\n",
      "\n",
      "Epoch [1/100], Batch [475/706], Loss: 0.6930\n",
      "\n",
      "Epoch [1/100], Batch [480/706], Loss: 0.6931\n",
      "\n",
      "Epoch [1/100], Batch [485/706], Loss: 0.6923\n",
      "\n",
      "Epoch [1/100], Batch [490/706], Loss: 0.6927\n",
      "\n",
      "Epoch [1/100], Batch [495/706], Loss: 0.6928\n",
      "\n",
      "Epoch [1/100], Batch [500/706], Loss: 0.6925\n",
      "\n",
      "Epoch [1/100], Batch [505/706], Loss: 0.6926\n",
      "\n",
      "Epoch [1/100], Batch [510/706], Loss: 0.6930\n",
      "\n",
      "Epoch [1/100], Batch [515/706], Loss: 0.6933\n",
      "\n",
      "Epoch [1/100], Batch [520/706], Loss: 0.6931\n",
      "\n",
      "Epoch [1/100], Batch [525/706], Loss: 0.6935\n",
      "\n",
      "Epoch [1/100], Batch [530/706], Loss: 0.6930\n",
      "\n",
      "Epoch [1/100], Batch [535/706], Loss: 0.6935\n",
      "\n",
      "Epoch [1/100], Batch [540/706], Loss: 0.6930\n",
      "\n",
      "Epoch [1/100], Batch [545/706], Loss: 0.6925\n",
      "\n",
      "Epoch [1/100], Batch [550/706], Loss: 0.6922\n",
      "\n",
      "Epoch [1/100], Batch [555/706], Loss: 0.6926\n",
      "\n",
      "Epoch [1/100], Batch [560/706], Loss: 0.6932\n",
      "\n",
      "Epoch [1/100], Batch [565/706], Loss: 0.6933\n",
      "\n",
      "Epoch [1/100], Batch [570/706], Loss: 0.6918\n",
      "\n",
      "Epoch [1/100], Batch [575/706], Loss: 0.6921\n",
      "\n",
      "Epoch [1/100], Batch [580/706], Loss: 0.6932\n",
      "\n",
      "Epoch [1/100], Batch [585/706], Loss: 0.6921\n",
      "\n",
      "Epoch [1/100], Batch [590/706], Loss: 0.6924\n",
      "\n",
      "Epoch [1/100], Batch [595/706], Loss: 0.6928\n",
      "\n",
      "Epoch [1/100], Batch [600/706], Loss: 0.6936\n",
      "\n",
      "Epoch [1/100], Batch [605/706], Loss: 0.6940\n",
      "\n",
      "Epoch [1/100], Batch [610/706], Loss: 0.6924\n",
      "\n",
      "Epoch [1/100], Batch [615/706], Loss: 0.6930\n",
      "\n",
      "Epoch [1/100], Batch [620/706], Loss: 0.6926\n",
      "\n",
      "Epoch [1/100], Batch [625/706], Loss: 0.6921\n",
      "\n",
      "Epoch [1/100], Batch [630/706], Loss: 0.6930\n",
      "\n",
      "Epoch [1/100], Batch [635/706], Loss: 0.6927\n",
      "\n",
      "Epoch [1/100], Batch [640/706], Loss: 0.6923\n",
      "\n",
      "Epoch [1/100], Batch [645/706], Loss: 0.6934\n",
      "\n",
      "Epoch [1/100], Batch [650/706], Loss: 0.6928\n",
      "\n",
      "Epoch [1/100], Batch [655/706], Loss: 0.6924\n",
      "\n",
      "Epoch [1/100], Batch [660/706], Loss: 0.6932\n",
      "\n",
      "Epoch [1/100], Batch [665/706], Loss: 0.6931\n",
      "\n",
      "Epoch [1/100], Batch [670/706], Loss: 0.6924\n",
      "\n",
      "Epoch [1/100], Batch [675/706], Loss: 0.6925\n",
      "\n",
      "Epoch [1/100], Batch [680/706], Loss: 0.6926\n",
      "\n",
      "Epoch [1/100], Batch [685/706], Loss: 0.6927\n",
      "\n",
      "Epoch [1/100], Batch [690/706], Loss: 0.6933\n",
      "\n",
      "Epoch [1/100], Batch [695/706], Loss: 0.6931\n",
      "\n",
      "Epoch [1/100], Batch [700/706], Loss: 0.6930\n",
      "\n",
      "Epoch [1/100], Batch [705/706], Loss: 0.6918\n",
      "\n",
      "Epoch [2/100], Batch [0/706], Loss: 0.6926\n",
      "\n",
      "Epoch [2/100], Batch [5/706], Loss: 0.6926\n",
      "\n",
      "Epoch [2/100], Batch [10/706], Loss: 0.6930\n",
      "\n",
      "Epoch [2/100], Batch [15/706], Loss: 0.6923\n",
      "\n",
      "Epoch [2/100], Batch [20/706], Loss: 0.6931\n",
      "\n",
      "Epoch [2/100], Batch [25/706], Loss: 0.6921\n",
      "\n",
      "Epoch [2/100], Batch [30/706], Loss: 0.6929\n",
      "\n",
      "Epoch [2/100], Batch [35/706], Loss: 0.6926\n",
      "\n",
      "Epoch [2/100], Batch [40/706], Loss: 0.6931\n",
      "\n",
      "Epoch [2/100], Batch [45/706], Loss: 0.6925\n",
      "\n",
      "Epoch [2/100], Batch [50/706], Loss: 0.6928\n",
      "\n",
      "Epoch [2/100], Batch [55/706], Loss: 0.6921\n",
      "\n",
      "Epoch [2/100], Batch [60/706], Loss: 0.6920\n",
      "\n",
      "Epoch [2/100], Batch [65/706], Loss: 0.6933\n",
      "\n",
      "Epoch [2/100], Batch [70/706], Loss: 0.6923\n",
      "\n",
      "Epoch [2/100], Batch [75/706], Loss: 0.6930\n",
      "\n",
      "Epoch [2/100], Batch [80/706], Loss: 0.6923\n",
      "\n",
      "Epoch [2/100], Batch [85/706], Loss: 0.6925\n",
      "\n",
      "Epoch [2/100], Batch [90/706], Loss: 0.6928\n",
      "\n",
      "Epoch [2/100], Batch [95/706], Loss: 0.6930\n",
      "\n",
      "Epoch [2/100], Batch [100/706], Loss: 0.6926\n",
      "\n",
      "Epoch [2/100], Batch [105/706], Loss: 0.6928\n",
      "\n",
      "Epoch [2/100], Batch [110/706], Loss: 0.6933\n",
      "\n",
      "Epoch [2/100], Batch [115/706], Loss: 0.6924\n",
      "\n",
      "Epoch [2/100], Batch [120/706], Loss: 0.6926\n",
      "\n",
      "Epoch [2/100], Batch [125/706], Loss: 0.6932\n",
      "\n",
      "Epoch [2/100], Batch [130/706], Loss: 0.6933\n",
      "\n",
      "Epoch [2/100], Batch [135/706], Loss: 0.6926\n",
      "\n",
      "Epoch [2/100], Batch [140/706], Loss: 0.6927\n",
      "\n",
      "Epoch [2/100], Batch [145/706], Loss: 0.6928\n",
      "\n",
      "Epoch [2/100], Batch [150/706], Loss: 0.6935\n",
      "\n",
      "Epoch [2/100], Batch [155/706], Loss: 0.6930\n",
      "\n",
      "Epoch [2/100], Batch [160/706], Loss: 0.6925\n",
      "\n",
      "Epoch [2/100], Batch [165/706], Loss: 0.6919\n",
      "\n",
      "Epoch [2/100], Batch [170/706], Loss: 0.6928\n",
      "\n",
      "Epoch [2/100], Batch [175/706], Loss: 0.6937\n",
      "\n",
      "Epoch [2/100], Batch [180/706], Loss: 0.6933\n",
      "\n",
      "Epoch [2/100], Batch [185/706], Loss: 0.6922\n",
      "\n",
      "Epoch [2/100], Batch [190/706], Loss: 0.6930\n",
      "\n",
      "Epoch [2/100], Batch [195/706], Loss: 0.6935\n",
      "\n",
      "Epoch [2/100], Batch [200/706], Loss: 0.6933\n",
      "\n",
      "Epoch [2/100], Batch [205/706], Loss: 0.6919\n",
      "\n",
      "Epoch [2/100], Batch [210/706], Loss: 0.6919\n",
      "\n",
      "Epoch [2/100], Batch [215/706], Loss: 0.6926\n",
      "\n",
      "Epoch [2/100], Batch [220/706], Loss: 0.6929\n",
      "\n",
      "Epoch [2/100], Batch [225/706], Loss: 0.6924\n",
      "\n",
      "Epoch [2/100], Batch [230/706], Loss: 0.6928\n",
      "\n",
      "Epoch [2/100], Batch [235/706], Loss: 0.6921\n",
      "\n",
      "Epoch [2/100], Batch [240/706], Loss: 0.6930\n",
      "\n",
      "Epoch [2/100], Batch [245/706], Loss: 0.6932\n",
      "\n",
      "Epoch [2/100], Batch [250/706], Loss: 0.6940\n",
      "\n",
      "Epoch [2/100], Batch [255/706], Loss: 0.6921\n",
      "\n",
      "Epoch [2/100], Batch [260/706], Loss: 0.6930\n",
      "\n",
      "Epoch [2/100], Batch [265/706], Loss: 0.6925\n",
      "\n",
      "Epoch [2/100], Batch [270/706], Loss: 0.6929\n",
      "\n",
      "Epoch [2/100], Batch [275/706], Loss: 0.6937\n",
      "\n",
      "Epoch [2/100], Batch [280/706], Loss: 0.6925\n",
      "\n",
      "Epoch [2/100], Batch [285/706], Loss: 0.6924\n",
      "\n",
      "Epoch [2/100], Batch [290/706], Loss: 0.6935\n",
      "\n",
      "Epoch [2/100], Batch [295/706], Loss: 0.6930\n",
      "\n",
      "Epoch [2/100], Batch [300/706], Loss: 0.6919\n",
      "\n",
      "Epoch [2/100], Batch [305/706], Loss: 0.6939\n",
      "\n",
      "Epoch [2/100], Batch [310/706], Loss: 0.6923\n",
      "\n",
      "Epoch [2/100], Batch [315/706], Loss: 0.6930\n",
      "\n",
      "Epoch [2/100], Batch [320/706], Loss: 0.6924\n",
      "\n",
      "Epoch [2/100], Batch [325/706], Loss: 0.6926\n",
      "\n",
      "Epoch [2/100], Batch [330/706], Loss: 0.6921\n",
      "\n",
      "Epoch [2/100], Batch [335/706], Loss: 0.6928\n",
      "\n",
      "Epoch [2/100], Batch [340/706], Loss: 0.6928\n",
      "\n",
      "Epoch [2/100], Batch [345/706], Loss: 0.6936\n",
      "\n",
      "Epoch [2/100], Batch [350/706], Loss: 0.6930\n",
      "\n",
      "Epoch [2/100], Batch [355/706], Loss: 0.6935\n",
      "\n",
      "Epoch [2/100], Batch [360/706], Loss: 0.6926\n",
      "\n",
      "Epoch [2/100], Batch [365/706], Loss: 0.6930\n",
      "\n",
      "Epoch [2/100], Batch [370/706], Loss: 0.6932\n",
      "\n",
      "Epoch [2/100], Batch [375/706], Loss: 0.6932\n",
      "\n",
      "Epoch [2/100], Batch [380/706], Loss: 0.6929\n",
      "\n",
      "Epoch [2/100], Batch [385/706], Loss: 0.6932\n",
      "\n",
      "Epoch [2/100], Batch [390/706], Loss: 0.6924\n",
      "\n",
      "Epoch [2/100], Batch [395/706], Loss: 0.6928\n",
      "\n",
      "Epoch [2/100], Batch [400/706], Loss: 0.6914\n",
      "\n",
      "Epoch [2/100], Batch [405/706], Loss: 0.6926\n",
      "\n",
      "Epoch [2/100], Batch [410/706], Loss: 0.6927\n",
      "\n",
      "Epoch [2/100], Batch [415/706], Loss: 0.6933\n",
      "\n",
      "Epoch [2/100], Batch [420/706], Loss: 0.6935\n",
      "\n",
      "Epoch [2/100], Batch [425/706], Loss: 0.6923\n",
      "\n",
      "Epoch [2/100], Batch [430/706], Loss: 0.6928\n",
      "\n",
      "Epoch [2/100], Batch [435/706], Loss: 0.6933\n",
      "\n",
      "Epoch [2/100], Batch [440/706], Loss: 0.6937\n",
      "\n",
      "Epoch [2/100], Batch [445/706], Loss: 0.6931\n",
      "\n",
      "Epoch [2/100], Batch [450/706], Loss: 0.6924\n",
      "\n",
      "Epoch [2/100], Batch [455/706], Loss: 0.6930\n",
      "\n",
      "Epoch [2/100], Batch [460/706], Loss: 0.6932\n",
      "\n",
      "Epoch [2/100], Batch [465/706], Loss: 0.6940\n",
      "\n",
      "Epoch [2/100], Batch [470/706], Loss: 0.6925\n",
      "\n",
      "Epoch [2/100], Batch [475/706], Loss: 0.6919\n",
      "\n",
      "Epoch [2/100], Batch [480/706], Loss: 0.6929\n",
      "\n",
      "Epoch [2/100], Batch [485/706], Loss: 0.6930\n",
      "\n",
      "Epoch [2/100], Batch [490/706], Loss: 0.6924\n",
      "\n",
      "Epoch [2/100], Batch [495/706], Loss: 0.6928\n",
      "\n",
      "Epoch [2/100], Batch [500/706], Loss: 0.6924\n",
      "\n",
      "Epoch [2/100], Batch [505/706], Loss: 0.6926\n",
      "\n",
      "Epoch [2/100], Batch [510/706], Loss: 0.6932\n",
      "\n",
      "Epoch [2/100], Batch [515/706], Loss: 0.6931\n",
      "\n",
      "Epoch [2/100], Batch [520/706], Loss: 0.6935\n",
      "\n",
      "Epoch [2/100], Batch [525/706], Loss: 0.6929\n",
      "\n",
      "Epoch [2/100], Batch [530/706], Loss: 0.6933\n",
      "\n",
      "Epoch [2/100], Batch [535/706], Loss: 0.6923\n",
      "\n",
      "Epoch [2/100], Batch [540/706], Loss: 0.6930\n",
      "\n",
      "Epoch [2/100], Batch [545/706], Loss: 0.6924\n",
      "\n",
      "Epoch [2/100], Batch [550/706], Loss: 0.6930\n",
      "\n",
      "Epoch [2/100], Batch [555/706], Loss: 0.6925\n",
      "\n",
      "Epoch [2/100], Batch [560/706], Loss: 0.6925\n",
      "\n",
      "Epoch [2/100], Batch [565/706], Loss: 0.6928\n",
      "\n",
      "Epoch [2/100], Batch [570/706], Loss: 0.6930\n",
      "\n",
      "Epoch [2/100], Batch [575/706], Loss: 0.6936\n",
      "\n",
      "Epoch [2/100], Batch [580/706], Loss: 0.6912\n",
      "\n",
      "Epoch [2/100], Batch [585/706], Loss: 0.6921\n",
      "\n",
      "Epoch [2/100], Batch [590/706], Loss: 0.6939\n",
      "\n",
      "Epoch [2/100], Batch [595/706], Loss: 0.6931\n",
      "\n",
      "Epoch [2/100], Batch [600/706], Loss: 0.6926\n",
      "\n",
      "Epoch [2/100], Batch [605/706], Loss: 0.6921\n",
      "\n",
      "Epoch [2/100], Batch [610/706], Loss: 0.6925\n",
      "\n",
      "Epoch [2/100], Batch [615/706], Loss: 0.6933\n",
      "\n",
      "Epoch [2/100], Batch [620/706], Loss: 0.6923\n",
      "\n",
      "Epoch [2/100], Batch [625/706], Loss: 0.6928\n",
      "\n",
      "Epoch [2/100], Batch [630/706], Loss: 0.6935\n",
      "\n",
      "Epoch [2/100], Batch [635/706], Loss: 0.6928\n",
      "\n",
      "Epoch [2/100], Batch [640/706], Loss: 0.6921\n",
      "\n",
      "Epoch [2/100], Batch [645/706], Loss: 0.6929\n",
      "\n",
      "Epoch [2/100], Batch [650/706], Loss: 0.6928\n",
      "\n",
      "Epoch [2/100], Batch [655/706], Loss: 0.6921\n",
      "\n",
      "Epoch [2/100], Batch [660/706], Loss: 0.6935\n",
      "\n",
      "Epoch [2/100], Batch [665/706], Loss: 0.6933\n",
      "\n",
      "Epoch [2/100], Batch [670/706], Loss: 0.6928\n",
      "\n",
      "Epoch [2/100], Batch [675/706], Loss: 0.6925\n",
      "\n",
      "Epoch [2/100], Batch [680/706], Loss: 0.6931\n",
      "\n",
      "Epoch [2/100], Batch [685/706], Loss: 0.6935\n",
      "\n",
      "Epoch [2/100], Batch [690/706], Loss: 0.6935\n",
      "\n",
      "Epoch [2/100], Batch [695/706], Loss: 0.6935\n",
      "\n",
      "Epoch [2/100], Batch [700/706], Loss: 0.6932\n",
      "\n",
      "Epoch [2/100], Batch [705/706], Loss: 0.6917\n",
      "\n",
      "Epoch [3/100], Batch [0/706], Loss: 0.6927\n",
      "\n",
      "Epoch [3/100], Batch [5/706], Loss: 0.6920\n",
      "\n",
      "Epoch [3/100], Batch [10/706], Loss: 0.6929\n",
      "\n",
      "Epoch [3/100], Batch [15/706], Loss: 0.6924\n",
      "\n",
      "Epoch [3/100], Batch [20/706], Loss: 0.6926\n",
      "\n",
      "Epoch [3/100], Batch [25/706], Loss: 0.6921\n",
      "\n",
      "Epoch [3/100], Batch [30/706], Loss: 0.6926\n",
      "\n",
      "Epoch [3/100], Batch [35/706], Loss: 0.6931\n",
      "\n",
      "Epoch [3/100], Batch [40/706], Loss: 0.6921\n",
      "\n",
      "Epoch [3/100], Batch [45/706], Loss: 0.6929\n",
      "\n",
      "Epoch [3/100], Batch [50/706], Loss: 0.6945\n",
      "\n",
      "Epoch [3/100], Batch [55/706], Loss: 0.6930\n",
      "\n",
      "Epoch [3/100], Batch [60/706], Loss: 0.6931\n",
      "\n",
      "Epoch [3/100], Batch [65/706], Loss: 0.6928\n",
      "\n",
      "Epoch [3/100], Batch [70/706], Loss: 0.6931\n",
      "\n",
      "Epoch [3/100], Batch [75/706], Loss: 0.6924\n",
      "\n",
      "Epoch [3/100], Batch [80/706], Loss: 0.6931\n",
      "\n",
      "Epoch [3/100], Batch [85/706], Loss: 0.6925\n",
      "\n",
      "Epoch [3/100], Batch [90/706], Loss: 0.6935\n",
      "\n",
      "Epoch [3/100], Batch [95/706], Loss: 0.6923\n",
      "\n",
      "Epoch [3/100], Batch [100/706], Loss: 0.6928\n",
      "\n",
      "Epoch [3/100], Batch [105/706], Loss: 0.6925\n",
      "\n",
      "Epoch [3/100], Batch [110/706], Loss: 0.6921\n",
      "\n",
      "Epoch [3/100], Batch [115/706], Loss: 0.6932\n",
      "\n",
      "Epoch [3/100], Batch [120/706], Loss: 0.6921\n",
      "\n",
      "Epoch [3/100], Batch [125/706], Loss: 0.6933\n",
      "\n",
      "Epoch [3/100], Batch [130/706], Loss: 0.6928\n",
      "\n",
      "Epoch [3/100], Batch [135/706], Loss: 0.6931\n",
      "\n",
      "Epoch [3/100], Batch [140/706], Loss: 0.6923\n",
      "\n",
      "Epoch [3/100], Batch [145/706], Loss: 0.6923\n",
      "\n",
      "Epoch [3/100], Batch [150/706], Loss: 0.6930\n",
      "\n",
      "Epoch [3/100], Batch [155/706], Loss: 0.6923\n",
      "\n",
      "Epoch [3/100], Batch [160/706], Loss: 0.6926\n",
      "\n",
      "Epoch [3/100], Batch [165/706], Loss: 0.6930\n",
      "\n",
      "Epoch [3/100], Batch [170/706], Loss: 0.6926\n",
      "\n",
      "Epoch [3/100], Batch [175/706], Loss: 0.6935\n",
      "\n",
      "Epoch [3/100], Batch [180/706], Loss: 0.6928\n",
      "\n",
      "Epoch [3/100], Batch [185/706], Loss: 0.6930\n",
      "\n",
      "Epoch [3/100], Batch [190/706], Loss: 0.6923\n",
      "\n",
      "Epoch [3/100], Batch [195/706], Loss: 0.6926\n",
      "\n",
      "Epoch [3/100], Batch [200/706], Loss: 0.6932\n",
      "\n",
      "Epoch [3/100], Batch [205/706], Loss: 0.6933\n",
      "\n",
      "Epoch [3/100], Batch [210/706], Loss: 0.6928\n",
      "\n",
      "Epoch [3/100], Batch [215/706], Loss: 0.6928\n",
      "\n",
      "Epoch [3/100], Batch [220/706], Loss: 0.6929\n",
      "\n",
      "Epoch [3/100], Batch [225/706], Loss: 0.6931\n",
      "\n",
      "Epoch [3/100], Batch [230/706], Loss: 0.6928\n",
      "\n",
      "Epoch [3/100], Batch [235/706], Loss: 0.6924\n",
      "\n",
      "Epoch [3/100], Batch [240/706], Loss: 0.6933\n",
      "\n",
      "Epoch [3/100], Batch [245/706], Loss: 0.6929\n",
      "\n",
      "Epoch [3/100], Batch [250/706], Loss: 0.6921\n",
      "\n",
      "Epoch [3/100], Batch [255/706], Loss: 0.6931\n",
      "\n",
      "Epoch [3/100], Batch [260/706], Loss: 0.6925\n",
      "\n",
      "Epoch [3/100], Batch [265/706], Loss: 0.6923\n",
      "\n",
      "Epoch [3/100], Batch [270/706], Loss: 0.6930\n",
      "\n",
      "Epoch [3/100], Batch [275/706], Loss: 0.6926\n",
      "\n",
      "Epoch [3/100], Batch [280/706], Loss: 0.6930\n",
      "\n",
      "Epoch [3/100], Batch [285/706], Loss: 0.6921\n",
      "\n",
      "Epoch [3/100], Batch [290/706], Loss: 0.6930\n",
      "\n",
      "Epoch [3/100], Batch [295/706], Loss: 0.6918\n",
      "\n",
      "Epoch [3/100], Batch [300/706], Loss: 0.6923\n",
      "\n",
      "Epoch [3/100], Batch [305/706], Loss: 0.6914\n",
      "\n",
      "Epoch [3/100], Batch [310/706], Loss: 0.6930\n",
      "\n",
      "Epoch [3/100], Batch [315/706], Loss: 0.6916\n",
      "\n",
      "Epoch [3/100], Batch [320/706], Loss: 0.6930\n",
      "\n",
      "Epoch [3/100], Batch [325/706], Loss: 0.6930\n",
      "\n",
      "Epoch [3/100], Batch [330/706], Loss: 0.6925\n",
      "\n",
      "Epoch [3/100], Batch [335/706], Loss: 0.6923\n",
      "\n",
      "Epoch [3/100], Batch [340/706], Loss: 0.6919\n",
      "\n",
      "Epoch [3/100], Batch [345/706], Loss: 0.6931\n",
      "\n",
      "Epoch [3/100], Batch [350/706], Loss: 0.6933\n",
      "\n",
      "Epoch [3/100], Batch [355/706], Loss: 0.6930\n",
      "\n",
      "Epoch [3/100], Batch [360/706], Loss: 0.6936\n",
      "\n",
      "Epoch [3/100], Batch [365/706], Loss: 0.6923\n",
      "\n",
      "Epoch [3/100], Batch [370/706], Loss: 0.6926\n",
      "\n",
      "Epoch [3/100], Batch [375/706], Loss: 0.6927\n",
      "\n",
      "Epoch [3/100], Batch [380/706], Loss: 0.6924\n",
      "\n",
      "Epoch [3/100], Batch [385/706], Loss: 0.6935\n",
      "\n",
      "Epoch [3/100], Batch [390/706], Loss: 0.6931\n",
      "\n",
      "Epoch [3/100], Batch [395/706], Loss: 0.6921\n",
      "\n",
      "Epoch [3/100], Batch [400/706], Loss: 0.6930\n",
      "\n",
      "Epoch [3/100], Batch [405/706], Loss: 0.6928\n",
      "\n",
      "Epoch [3/100], Batch [410/706], Loss: 0.6922\n",
      "\n",
      "Epoch [3/100], Batch [415/706], Loss: 0.6923\n",
      "\n",
      "Epoch [3/100], Batch [420/706], Loss: 0.6934\n",
      "\n",
      "Epoch [3/100], Batch [425/706], Loss: 0.6927\n",
      "\n",
      "Epoch [3/100], Batch [430/706], Loss: 0.6927\n",
      "\n",
      "Epoch [3/100], Batch [435/706], Loss: 0.6935\n",
      "\n",
      "Epoch [3/100], Batch [440/706], Loss: 0.6928\n",
      "\n",
      "Epoch [3/100], Batch [445/706], Loss: 0.6932\n",
      "\n",
      "Epoch [3/100], Batch [450/706], Loss: 0.6923\n",
      "\n",
      "Epoch [3/100], Batch [455/706], Loss: 0.6927\n",
      "\n",
      "Epoch [3/100], Batch [460/706], Loss: 0.6926\n",
      "\n",
      "Epoch [3/100], Batch [465/706], Loss: 0.6926\n",
      "\n",
      "Epoch [3/100], Batch [470/706], Loss: 0.6928\n",
      "\n",
      "Epoch [3/100], Batch [475/706], Loss: 0.6924\n",
      "\n",
      "Epoch [3/100], Batch [480/706], Loss: 0.6929\n",
      "\n",
      "Epoch [3/100], Batch [485/706], Loss: 0.6928\n",
      "\n",
      "Epoch [3/100], Batch [490/706], Loss: 0.6940\n",
      "\n",
      "Epoch [3/100], Batch [495/706], Loss: 0.6926\n",
      "\n",
      "Epoch [3/100], Batch [500/706], Loss: 0.6925\n",
      "\n",
      "Epoch [3/100], Batch [505/706], Loss: 0.6925\n",
      "\n",
      "Epoch [3/100], Batch [510/706], Loss: 0.6927\n",
      "\n",
      "Epoch [3/100], Batch [515/706], Loss: 0.6934\n",
      "\n",
      "Epoch [3/100], Batch [520/706], Loss: 0.6931\n",
      "\n",
      "Epoch [3/100], Batch [525/706], Loss: 0.6926\n",
      "\n",
      "Epoch [3/100], Batch [530/706], Loss: 0.6926\n",
      "\n",
      "Epoch [3/100], Batch [535/706], Loss: 0.6930\n",
      "\n",
      "Epoch [3/100], Batch [540/706], Loss: 0.6926\n",
      "\n",
      "Epoch [3/100], Batch [545/706], Loss: 0.6929\n",
      "\n",
      "Epoch [3/100], Batch [550/706], Loss: 0.6930\n",
      "\n",
      "Epoch [3/100], Batch [555/706], Loss: 0.6933\n",
      "\n",
      "Epoch [3/100], Batch [560/706], Loss: 0.6925\n",
      "\n",
      "Epoch [3/100], Batch [565/706], Loss: 0.6931\n",
      "\n",
      "Epoch [3/100], Batch [570/706], Loss: 0.6933\n",
      "\n",
      "Epoch [3/100], Batch [575/706], Loss: 0.6930\n",
      "\n",
      "Epoch [3/100], Batch [580/706], Loss: 0.6924\n",
      "\n",
      "Epoch [3/100], Batch [585/706], Loss: 0.6933\n",
      "\n",
      "Epoch [3/100], Batch [590/706], Loss: 0.6924\n",
      "\n",
      "Epoch [3/100], Batch [595/706], Loss: 0.6928\n",
      "\n",
      "Epoch [3/100], Batch [600/706], Loss: 0.6921\n",
      "\n",
      "Epoch [3/100], Batch [605/706], Loss: 0.6933\n",
      "\n",
      "Epoch [3/100], Batch [610/706], Loss: 0.6935\n",
      "\n",
      "Epoch [3/100], Batch [615/706], Loss: 0.6925\n",
      "\n",
      "Epoch [3/100], Batch [620/706], Loss: 0.6918\n",
      "\n",
      "Epoch [3/100], Batch [625/706], Loss: 0.6935\n",
      "\n",
      "Epoch [3/100], Batch [630/706], Loss: 0.6928\n",
      "\n",
      "Epoch [3/100], Batch [635/706], Loss: 0.6924\n",
      "\n",
      "Epoch [3/100], Batch [640/706], Loss: 0.6935\n",
      "\n",
      "Epoch [3/100], Batch [645/706], Loss: 0.6925\n",
      "\n",
      "Epoch [3/100], Batch [650/706], Loss: 0.6935\n",
      "\n",
      "Epoch [3/100], Batch [655/706], Loss: 0.6932\n",
      "\n",
      "Epoch [3/100], Batch [660/706], Loss: 0.6937\n",
      "\n",
      "Epoch [3/100], Batch [665/706], Loss: 0.6926\n",
      "\n",
      "Epoch [3/100], Batch [670/706], Loss: 0.6928\n",
      "\n",
      "Epoch [3/100], Batch [675/706], Loss: 0.6921\n",
      "\n",
      "Epoch [3/100], Batch [680/706], Loss: 0.6937\n",
      "\n",
      "Epoch [3/100], Batch [685/706], Loss: 0.6919\n",
      "\n",
      "Epoch [3/100], Batch [690/706], Loss: 0.6937\n",
      "\n",
      "Epoch [3/100], Batch [695/706], Loss: 0.6928\n",
      "\n",
      "Epoch [3/100], Batch [700/706], Loss: 0.6937\n",
      "\n",
      "Epoch [3/100], Batch [705/706], Loss: 0.6931\n",
      "\n",
      "Epoch [4/100], Batch [0/706], Loss: 0.6921\n",
      "\n",
      "Epoch [4/100], Batch [5/706], Loss: 0.6930\n",
      "\n",
      "Epoch [4/100], Batch [10/706], Loss: 0.6923\n",
      "\n",
      "Epoch [4/100], Batch [15/706], Loss: 0.6937\n",
      "\n",
      "Epoch [4/100], Batch [20/706], Loss: 0.6925\n",
      "\n",
      "Epoch [4/100], Batch [25/706], Loss: 0.6926\n",
      "\n",
      "Epoch [4/100], Batch [30/706], Loss: 0.6926\n",
      "\n",
      "Epoch [4/100], Batch [35/706], Loss: 0.6921\n",
      "\n",
      "Epoch [4/100], Batch [40/706], Loss: 0.6926\n",
      "\n",
      "Epoch [4/100], Batch [45/706], Loss: 0.6927\n",
      "\n",
      "Epoch [4/100], Batch [50/706], Loss: 0.6926\n",
      "\n",
      "Epoch [4/100], Batch [55/706], Loss: 0.6933\n",
      "\n",
      "Epoch [4/100], Batch [60/706], Loss: 0.6925\n",
      "\n",
      "Epoch [4/100], Batch [65/706], Loss: 0.6923\n",
      "\n",
      "Epoch [4/100], Batch [70/706], Loss: 0.6920\n",
      "\n",
      "Epoch [4/100], Batch [75/706], Loss: 0.6928\n",
      "\n",
      "Epoch [4/100], Batch [80/706], Loss: 0.6932\n",
      "\n",
      "Epoch [4/100], Batch [85/706], Loss: 0.6923\n",
      "\n",
      "Epoch [4/100], Batch [90/706], Loss: 0.6933\n",
      "\n",
      "Epoch [4/100], Batch [95/706], Loss: 0.6935\n",
      "\n",
      "Epoch [4/100], Batch [100/706], Loss: 0.6928\n",
      "\n",
      "Epoch [4/100], Batch [105/706], Loss: 0.6932\n",
      "\n",
      "Epoch [4/100], Batch [110/706], Loss: 0.6930\n",
      "\n",
      "Epoch [4/100], Batch [115/706], Loss: 0.6931\n",
      "\n",
      "Epoch [4/100], Batch [120/706], Loss: 0.6925\n",
      "\n",
      "Epoch [4/100], Batch [125/706], Loss: 0.6928\n",
      "\n",
      "Epoch [4/100], Batch [130/706], Loss: 0.6928\n",
      "\n",
      "Epoch [4/100], Batch [135/706], Loss: 0.6931\n",
      "\n",
      "Epoch [4/100], Batch [140/706], Loss: 0.6924\n",
      "\n",
      "Epoch [4/100], Batch [145/706], Loss: 0.6921\n",
      "\n",
      "Epoch [4/100], Batch [150/706], Loss: 0.6923\n",
      "\n",
      "Epoch [4/100], Batch [155/706], Loss: 0.6925\n",
      "\n",
      "Epoch [4/100], Batch [160/706], Loss: 0.6925\n",
      "\n",
      "Epoch [4/100], Batch [165/706], Loss: 0.6924\n",
      "\n",
      "Epoch [4/100], Batch [170/706], Loss: 0.6926\n",
      "\n",
      "Epoch [4/100], Batch [175/706], Loss: 0.6923\n",
      "\n",
      "Epoch [4/100], Batch [180/706], Loss: 0.6925\n",
      "\n",
      "Epoch [4/100], Batch [185/706], Loss: 0.6926\n",
      "\n",
      "Epoch [4/100], Batch [190/706], Loss: 0.6929\n",
      "\n",
      "Epoch [4/100], Batch [195/706], Loss: 0.6933\n",
      "\n",
      "Epoch [4/100], Batch [200/706], Loss: 0.6923\n",
      "\n",
      "Epoch [4/100], Batch [205/706], Loss: 0.6931\n",
      "\n",
      "Epoch [4/100], Batch [210/706], Loss: 0.6927\n",
      "\n",
      "Epoch [4/100], Batch [215/706], Loss: 0.6931\n",
      "\n",
      "Epoch [4/100], Batch [220/706], Loss: 0.6930\n",
      "\n",
      "Epoch [4/100], Batch [225/706], Loss: 0.6917\n",
      "\n",
      "Epoch [4/100], Batch [230/706], Loss: 0.6919\n",
      "\n",
      "Epoch [4/100], Batch [235/706], Loss: 0.6933\n",
      "\n",
      "Epoch [4/100], Batch [240/706], Loss: 0.6926\n",
      "\n",
      "Epoch [4/100], Batch [245/706], Loss: 0.6940\n",
      "\n",
      "Epoch [4/100], Batch [250/706], Loss: 0.6928\n",
      "\n",
      "Epoch [4/100], Batch [255/706], Loss: 0.6928\n",
      "\n",
      "Epoch [4/100], Batch [260/706], Loss: 0.6930\n",
      "\n",
      "Epoch [4/100], Batch [265/706], Loss: 0.6925\n",
      "\n",
      "Epoch [4/100], Batch [270/706], Loss: 0.6930\n",
      "\n",
      "Epoch [4/100], Batch [275/706], Loss: 0.6926\n",
      "\n",
      "Epoch [4/100], Batch [280/706], Loss: 0.6921\n",
      "\n",
      "Epoch [4/100], Batch [285/706], Loss: 0.6928\n",
      "\n",
      "Epoch [4/100], Batch [290/706], Loss: 0.6931\n",
      "\n",
      "Epoch [4/100], Batch [295/706], Loss: 0.6938\n",
      "\n",
      "Epoch [4/100], Batch [300/706], Loss: 0.6928\n",
      "\n",
      "Epoch [4/100], Batch [305/706], Loss: 0.6920\n",
      "\n",
      "Epoch [4/100], Batch [310/706], Loss: 0.6930\n",
      "\n",
      "Epoch [4/100], Batch [315/706], Loss: 0.6926\n",
      "\n",
      "Epoch [4/100], Batch [320/706], Loss: 0.6930\n",
      "\n",
      "Epoch [4/100], Batch [325/706], Loss: 0.6921\n",
      "\n",
      "Epoch [4/100], Batch [330/706], Loss: 0.6930\n",
      "\n",
      "Epoch [4/100], Batch [335/706], Loss: 0.6933\n",
      "\n",
      "Epoch [4/100], Batch [340/706], Loss: 0.6935\n",
      "\n",
      "Epoch [4/100], Batch [345/706], Loss: 0.6926\n",
      "\n",
      "Epoch [4/100], Batch [350/706], Loss: 0.6931\n",
      "\n",
      "Epoch [4/100], Batch [355/706], Loss: 0.6933\n",
      "\n",
      "Epoch [4/100], Batch [360/706], Loss: 0.6921\n",
      "\n",
      "Epoch [4/100], Batch [365/706], Loss: 0.6928\n",
      "\n",
      "Epoch [4/100], Batch [370/706], Loss: 0.6935\n",
      "\n",
      "Epoch [4/100], Batch [375/706], Loss: 0.6933\n",
      "\n",
      "Epoch [4/100], Batch [380/706], Loss: 0.6936\n",
      "\n",
      "Epoch [4/100], Batch [385/706], Loss: 0.6928\n",
      "\n",
      "Epoch [4/100], Batch [390/706], Loss: 0.6935\n",
      "\n",
      "Epoch [4/100], Batch [395/706], Loss: 0.6926\n",
      "\n",
      "Epoch [4/100], Batch [400/706], Loss: 0.6935\n",
      "\n",
      "Epoch [4/100], Batch [405/706], Loss: 0.6923\n",
      "\n",
      "Epoch [4/100], Batch [410/706], Loss: 0.6919\n",
      "\n",
      "Epoch [4/100], Batch [415/706], Loss: 0.6922\n",
      "\n",
      "Epoch [4/100], Batch [420/706], Loss: 0.6928\n",
      "\n",
      "Epoch [4/100], Batch [425/706], Loss: 0.6924\n",
      "\n",
      "Epoch [4/100], Batch [430/706], Loss: 0.6928\n",
      "\n",
      "Epoch [4/100], Batch [435/706], Loss: 0.6932\n",
      "\n",
      "Epoch [4/100], Batch [440/706], Loss: 0.6930\n",
      "\n",
      "Epoch [4/100], Batch [445/706], Loss: 0.6923\n",
      "\n",
      "Epoch [4/100], Batch [450/706], Loss: 0.6925\n",
      "\n",
      "Epoch [4/100], Batch [455/706], Loss: 0.6933\n",
      "\n",
      "Epoch [4/100], Batch [460/706], Loss: 0.6930\n",
      "\n",
      "Epoch [4/100], Batch [465/706], Loss: 0.6923\n",
      "\n",
      "Epoch [4/100], Batch [470/706], Loss: 0.6933\n",
      "\n",
      "Epoch [4/100], Batch [475/706], Loss: 0.6930\n",
      "\n",
      "Epoch [4/100], Batch [480/706], Loss: 0.6931\n",
      "\n",
      "Epoch [4/100], Batch [485/706], Loss: 0.6924\n",
      "\n",
      "Epoch [4/100], Batch [490/706], Loss: 0.6926\n",
      "\n",
      "Epoch [4/100], Batch [495/706], Loss: 0.6918\n",
      "\n",
      "Epoch [4/100], Batch [500/706], Loss: 0.6930\n",
      "\n",
      "Epoch [4/100], Batch [505/706], Loss: 0.6924\n",
      "\n",
      "Epoch [4/100], Batch [510/706], Loss: 0.6928\n",
      "\n",
      "Epoch [4/100], Batch [515/706], Loss: 0.6926\n",
      "\n",
      "Epoch [4/100], Batch [520/706], Loss: 0.6925\n",
      "\n",
      "Epoch [4/100], Batch [525/706], Loss: 0.6926\n",
      "\n",
      "Epoch [4/100], Batch [530/706], Loss: 0.6929\n",
      "\n",
      "Epoch [4/100], Batch [535/706], Loss: 0.6931\n",
      "\n",
      "Epoch [4/100], Batch [540/706], Loss: 0.6933\n",
      "\n",
      "Epoch [4/100], Batch [545/706], Loss: 0.6923\n",
      "\n",
      "Epoch [4/100], Batch [550/706], Loss: 0.6933\n",
      "\n",
      "Epoch [4/100], Batch [555/706], Loss: 0.6929\n",
      "\n",
      "Epoch [4/100], Batch [560/706], Loss: 0.6928\n",
      "\n",
      "Epoch [4/100], Batch [565/706], Loss: 0.6921\n",
      "\n",
      "Epoch [4/100], Batch [570/706], Loss: 0.6935\n",
      "\n",
      "Epoch [4/100], Batch [575/706], Loss: 0.6930\n",
      "\n",
      "Epoch [4/100], Batch [580/706], Loss: 0.6925\n",
      "\n",
      "Epoch [4/100], Batch [585/706], Loss: 0.6928\n",
      "\n",
      "Epoch [4/100], Batch [590/706], Loss: 0.6930\n",
      "\n",
      "Epoch [4/100], Batch [595/706], Loss: 0.6932\n",
      "\n",
      "Epoch [4/100], Batch [600/706], Loss: 0.6937\n",
      "\n",
      "Epoch [4/100], Batch [605/706], Loss: 0.6923\n",
      "\n",
      "Epoch [4/100], Batch [610/706], Loss: 0.6921\n",
      "\n",
      "Epoch [4/100], Batch [615/706], Loss: 0.6933\n",
      "\n",
      "Epoch [4/100], Batch [620/706], Loss: 0.6934\n",
      "\n",
      "Epoch [4/100], Batch [625/706], Loss: 0.6931\n",
      "\n",
      "Epoch [4/100], Batch [630/706], Loss: 0.6931\n",
      "\n",
      "Epoch [4/100], Batch [635/706], Loss: 0.6927\n",
      "\n",
      "Epoch [4/100], Batch [640/706], Loss: 0.6921\n",
      "\n",
      "Epoch [4/100], Batch [645/706], Loss: 0.6923\n",
      "\n",
      "Epoch [4/100], Batch [650/706], Loss: 0.6921\n",
      "\n",
      "Epoch [4/100], Batch [655/706], Loss: 0.6928\n",
      "\n",
      "Epoch [4/100], Batch [660/706], Loss: 0.6926\n",
      "\n",
      "Epoch [4/100], Batch [665/706], Loss: 0.6923\n",
      "\n",
      "Epoch [4/100], Batch [670/706], Loss: 0.6924\n",
      "\n",
      "Epoch [4/100], Batch [675/706], Loss: 0.6920\n",
      "\n",
      "Epoch [4/100], Batch [680/706], Loss: 0.6924\n",
      "\n",
      "Epoch [4/100], Batch [685/706], Loss: 0.6923\n",
      "\n",
      "Epoch [4/100], Batch [690/706], Loss: 0.6921\n",
      "\n",
      "Epoch [4/100], Batch [695/706], Loss: 0.6923\n",
      "\n",
      "Epoch [4/100], Batch [700/706], Loss: 0.6923\n",
      "\n",
      "Epoch [4/100], Batch [705/706], Loss: 0.6917\n",
      "\n",
      "Epoch [5/100], Batch [0/706], Loss: 0.6933\n",
      "\n",
      "Epoch [5/100], Batch [5/706], Loss: 0.6931\n",
      "\n",
      "Epoch [5/100], Batch [10/706], Loss: 0.6923\n",
      "\n",
      "Epoch [5/100], Batch [15/706], Loss: 0.6924\n",
      "\n",
      "Epoch [5/100], Batch [20/706], Loss: 0.6926\n",
      "\n",
      "Epoch [5/100], Batch [25/706], Loss: 0.6928\n",
      "\n",
      "Epoch [5/100], Batch [30/706], Loss: 0.6934\n",
      "\n",
      "Epoch [5/100], Batch [35/706], Loss: 0.6925\n",
      "\n",
      "Epoch [5/100], Batch [40/706], Loss: 0.6919\n",
      "\n",
      "Epoch [5/100], Batch [45/706], Loss: 0.6929\n",
      "\n",
      "Epoch [5/100], Batch [50/706], Loss: 0.6926\n",
      "\n",
      "Epoch [5/100], Batch [55/706], Loss: 0.6926\n",
      "\n",
      "Epoch [5/100], Batch [60/706], Loss: 0.6932\n",
      "\n",
      "Epoch [5/100], Batch [65/706], Loss: 0.6923\n",
      "\n",
      "Epoch [5/100], Batch [70/706], Loss: 0.6926\n",
      "\n",
      "Epoch [5/100], Batch [75/706], Loss: 0.6935\n",
      "\n",
      "Epoch [5/100], Batch [80/706], Loss: 0.6919\n",
      "\n",
      "Epoch [5/100], Batch [85/706], Loss: 0.6923\n",
      "\n",
      "Epoch [5/100], Batch [90/706], Loss: 0.6930\n",
      "\n",
      "Epoch [5/100], Batch [95/706], Loss: 0.6924\n",
      "\n",
      "Epoch [5/100], Batch [100/706], Loss: 0.6929\n",
      "\n",
      "Epoch [5/100], Batch [105/706], Loss: 0.6940\n",
      "\n",
      "Epoch [5/100], Batch [110/706], Loss: 0.6928\n",
      "\n",
      "Epoch [5/100], Batch [115/706], Loss: 0.6931\n",
      "\n",
      "Epoch [5/100], Batch [120/706], Loss: 0.6928\n",
      "\n",
      "Epoch [5/100], Batch [125/706], Loss: 0.6919\n",
      "\n",
      "Epoch [5/100], Batch [130/706], Loss: 0.6928\n",
      "\n",
      "Epoch [5/100], Batch [135/706], Loss: 0.6940\n",
      "\n",
      "Epoch [5/100], Batch [140/706], Loss: 0.6926\n",
      "\n",
      "Epoch [5/100], Batch [145/706], Loss: 0.6925\n",
      "\n",
      "Epoch [5/100], Batch [150/706], Loss: 0.6935\n",
      "\n",
      "Epoch [5/100], Batch [155/706], Loss: 0.6923\n",
      "\n",
      "Epoch [5/100], Batch [160/706], Loss: 0.6933\n",
      "\n",
      "Epoch [5/100], Batch [165/706], Loss: 0.6926\n",
      "\n",
      "Epoch [5/100], Batch [170/706], Loss: 0.6927\n",
      "\n",
      "Epoch [5/100], Batch [175/706], Loss: 0.6928\n",
      "\n",
      "Epoch [5/100], Batch [180/706], Loss: 0.6919\n",
      "\n",
      "Epoch [5/100], Batch [185/706], Loss: 0.6933\n",
      "\n",
      "Epoch [5/100], Batch [190/706], Loss: 0.6926\n",
      "\n",
      "Epoch [5/100], Batch [195/706], Loss: 0.6919\n",
      "\n",
      "Epoch [5/100], Batch [200/706], Loss: 0.6918\n",
      "\n",
      "Epoch [5/100], Batch [205/706], Loss: 0.6932\n",
      "\n",
      "Epoch [5/100], Batch [210/706], Loss: 0.6919\n",
      "\n",
      "Epoch [5/100], Batch [215/706], Loss: 0.6928\n",
      "\n",
      "Epoch [5/100], Batch [220/706], Loss: 0.6934\n",
      "\n",
      "Epoch [5/100], Batch [225/706], Loss: 0.6929\n",
      "\n",
      "Epoch [5/100], Batch [230/706], Loss: 0.6925\n",
      "\n",
      "Epoch [5/100], Batch [235/706], Loss: 0.6916\n",
      "\n",
      "Epoch [5/100], Batch [240/706], Loss: 0.6923\n",
      "\n",
      "Epoch [5/100], Batch [245/706], Loss: 0.6935\n",
      "\n",
      "Epoch [5/100], Batch [250/706], Loss: 0.6928\n",
      "\n",
      "Epoch [5/100], Batch [255/706], Loss: 0.6923\n",
      "\n",
      "Epoch [5/100], Batch [260/706], Loss: 0.6924\n",
      "\n",
      "Epoch [5/100], Batch [265/706], Loss: 0.6930\n",
      "\n",
      "Epoch [5/100], Batch [270/706], Loss: 0.6935\n",
      "\n",
      "Epoch [5/100], Batch [275/706], Loss: 0.6922\n",
      "\n",
      "Epoch [5/100], Batch [280/706], Loss: 0.6930\n",
      "\n",
      "Epoch [5/100], Batch [285/706], Loss: 0.6932\n",
      "\n",
      "Epoch [5/100], Batch [290/706], Loss: 0.6924\n",
      "\n",
      "Epoch [5/100], Batch [295/706], Loss: 0.6933\n",
      "\n",
      "Epoch [5/100], Batch [300/706], Loss: 0.6937\n",
      "\n",
      "Epoch [5/100], Batch [305/706], Loss: 0.6937\n",
      "\n",
      "Epoch [5/100], Batch [310/706], Loss: 0.6926\n",
      "\n",
      "Epoch [5/100], Batch [315/706], Loss: 0.6926\n",
      "\n",
      "Epoch [5/100], Batch [320/706], Loss: 0.6926\n",
      "\n",
      "Epoch [5/100], Batch [325/706], Loss: 0.6915\n",
      "\n",
      "Epoch [5/100], Batch [330/706], Loss: 0.6925\n",
      "\n",
      "Epoch [5/100], Batch [335/706], Loss: 0.6930\n",
      "\n",
      "Epoch [5/100], Batch [340/706], Loss: 0.6931\n",
      "\n",
      "Epoch [5/100], Batch [345/706], Loss: 0.6926\n",
      "\n",
      "Epoch [5/100], Batch [350/706], Loss: 0.6928\n",
      "\n",
      "Epoch [5/100], Batch [355/706], Loss: 0.6926\n",
      "\n",
      "Epoch [5/100], Batch [360/706], Loss: 0.6930\n",
      "\n",
      "Epoch [5/100], Batch [365/706], Loss: 0.6938\n",
      "\n",
      "Epoch [5/100], Batch [370/706], Loss: 0.6935\n",
      "\n",
      "Epoch [5/100], Batch [375/706], Loss: 0.6919\n",
      "\n",
      "Epoch [5/100], Batch [380/706], Loss: 0.6926\n",
      "\n",
      "Epoch [5/100], Batch [385/706], Loss: 0.6925\n",
      "\n",
      "Epoch [5/100], Batch [390/706], Loss: 0.6919\n",
      "\n",
      "Epoch [5/100], Batch [395/706], Loss: 0.6930\n",
      "\n",
      "Epoch [5/100], Batch [400/706], Loss: 0.6930\n",
      "\n",
      "Epoch [5/100], Batch [405/706], Loss: 0.6922\n",
      "\n",
      "Epoch [5/100], Batch [410/706], Loss: 0.6925\n",
      "\n",
      "Epoch [5/100], Batch [415/706], Loss: 0.6923\n",
      "\n",
      "Epoch [5/100], Batch [420/706], Loss: 0.6930\n",
      "\n",
      "Epoch [5/100], Batch [425/706], Loss: 0.6926\n",
      "\n",
      "Epoch [5/100], Batch [430/706], Loss: 0.6932\n",
      "\n",
      "Epoch [5/100], Batch [435/706], Loss: 0.6924\n",
      "\n",
      "Epoch [5/100], Batch [440/706], Loss: 0.6921\n",
      "\n",
      "Epoch [5/100], Batch [445/706], Loss: 0.6931\n",
      "\n",
      "Epoch [5/100], Batch [450/706], Loss: 0.6919\n",
      "\n",
      "Epoch [5/100], Batch [455/706], Loss: 0.6923\n",
      "\n",
      "Epoch [5/100], Batch [460/706], Loss: 0.6937\n",
      "\n",
      "Epoch [5/100], Batch [465/706], Loss: 0.6933\n",
      "\n",
      "Epoch [5/100], Batch [470/706], Loss: 0.6923\n",
      "\n",
      "Epoch [5/100], Batch [475/706], Loss: 0.6925\n",
      "\n",
      "Epoch [5/100], Batch [480/706], Loss: 0.6926\n",
      "\n",
      "Epoch [5/100], Batch [485/706], Loss: 0.6934\n",
      "\n",
      "Epoch [5/100], Batch [490/706], Loss: 0.6933\n",
      "\n",
      "Epoch [5/100], Batch [495/706], Loss: 0.6929\n",
      "\n",
      "Epoch [5/100], Batch [500/706], Loss: 0.6931\n",
      "\n",
      "Epoch [5/100], Batch [505/706], Loss: 0.6928\n",
      "\n",
      "Epoch [5/100], Batch [510/706], Loss: 0.6926\n",
      "\n",
      "Epoch [5/100], Batch [515/706], Loss: 0.6926\n",
      "\n",
      "Epoch [5/100], Batch [520/706], Loss: 0.6937\n",
      "\n",
      "Epoch [5/100], Batch [525/706], Loss: 0.6924\n",
      "\n",
      "Epoch [5/100], Batch [530/706], Loss: 0.6931\n",
      "\n",
      "Epoch [5/100], Batch [535/706], Loss: 0.6933\n",
      "\n",
      "Epoch [5/100], Batch [540/706], Loss: 0.6929\n",
      "\n",
      "Epoch [5/100], Batch [545/706], Loss: 0.6926\n",
      "\n",
      "Epoch [5/100], Batch [550/706], Loss: 0.6928\n",
      "\n",
      "Epoch [5/100], Batch [555/706], Loss: 0.6935\n",
      "\n",
      "Epoch [5/100], Batch [560/706], Loss: 0.6925\n",
      "\n",
      "Epoch [5/100], Batch [565/706], Loss: 0.6926\n",
      "\n",
      "Epoch [5/100], Batch [570/706], Loss: 0.6922\n",
      "\n",
      "Epoch [5/100], Batch [575/706], Loss: 0.6921\n",
      "\n",
      "Epoch [5/100], Batch [580/706], Loss: 0.6927\n",
      "\n",
      "Epoch [5/100], Batch [585/706], Loss: 0.6926\n",
      "\n",
      "Epoch [5/100], Batch [590/706], Loss: 0.6928\n",
      "\n",
      "Epoch [5/100], Batch [595/706], Loss: 0.6925\n",
      "\n",
      "Epoch [5/100], Batch [600/706], Loss: 0.6935\n",
      "\n",
      "Epoch [5/100], Batch [605/706], Loss: 0.6924\n",
      "\n",
      "Epoch [5/100], Batch [610/706], Loss: 0.6932\n",
      "\n",
      "Epoch [5/100], Batch [615/706], Loss: 0.6930\n",
      "\n",
      "Epoch [5/100], Batch [620/706], Loss: 0.6932\n",
      "\n",
      "Epoch [5/100], Batch [625/706], Loss: 0.6925\n",
      "\n",
      "Epoch [5/100], Batch [630/706], Loss: 0.6925\n",
      "\n",
      "Epoch [5/100], Batch [635/706], Loss: 0.6926\n",
      "\n",
      "Epoch [5/100], Batch [640/706], Loss: 0.6934\n",
      "\n",
      "Epoch [5/100], Batch [645/706], Loss: 0.6932\n",
      "\n",
      "Epoch [5/100], Batch [650/706], Loss: 0.6921\n",
      "\n",
      "Epoch [5/100], Batch [655/706], Loss: 0.6926\n",
      "\n",
      "Epoch [5/100], Batch [660/706], Loss: 0.6927\n",
      "\n",
      "Epoch [5/100], Batch [665/706], Loss: 0.6919\n",
      "\n",
      "Epoch [5/100], Batch [670/706], Loss: 0.6923\n",
      "\n",
      "Epoch [5/100], Batch [675/706], Loss: 0.6928\n",
      "\n",
      "Epoch [5/100], Batch [680/706], Loss: 0.6932\n",
      "\n",
      "Epoch [5/100], Batch [685/706], Loss: 0.6925\n",
      "\n",
      "Epoch [5/100], Batch [690/706], Loss: 0.6931\n",
      "\n",
      "Epoch [5/100], Batch [695/706], Loss: 0.6931\n",
      "\n",
      "Epoch [5/100], Batch [700/706], Loss: 0.6925\n",
      "\n",
      "Epoch [5/100], Batch [705/706], Loss: 0.6903\n",
      "\n",
      "Epoch [6/100], Batch [0/706], Loss: 0.6921\n",
      "\n",
      "Epoch [6/100], Batch [5/706], Loss: 0.6923\n",
      "\n",
      "Epoch [6/100], Batch [10/706], Loss: 0.6936\n",
      "\n",
      "Epoch [6/100], Batch [15/706], Loss: 0.6933\n",
      "\n",
      "Epoch [6/100], Batch [20/706], Loss: 0.6921\n",
      "\n",
      "Epoch [6/100], Batch [25/706], Loss: 0.6921\n",
      "\n",
      "Epoch [6/100], Batch [30/706], Loss: 0.6926\n",
      "\n",
      "Epoch [6/100], Batch [35/706], Loss: 0.6921\n",
      "\n",
      "Epoch [6/100], Batch [40/706], Loss: 0.6929\n",
      "\n",
      "Epoch [6/100], Batch [45/706], Loss: 0.6923\n",
      "\n",
      "Epoch [6/100], Batch [50/706], Loss: 0.6930\n",
      "\n",
      "Epoch [6/100], Batch [55/706], Loss: 0.6931\n",
      "\n",
      "Epoch [6/100], Batch [60/706], Loss: 0.6923\n",
      "\n",
      "Epoch [6/100], Batch [65/706], Loss: 0.6936\n",
      "\n",
      "Epoch [6/100], Batch [70/706], Loss: 0.6930\n",
      "\n",
      "Epoch [6/100], Batch [75/706], Loss: 0.6930\n",
      "\n",
      "Epoch [6/100], Batch [80/706], Loss: 0.6932\n",
      "\n",
      "Epoch [6/100], Batch [85/706], Loss: 0.6926\n",
      "\n",
      "Epoch [6/100], Batch [90/706], Loss: 0.6926\n",
      "\n",
      "Epoch [6/100], Batch [95/706], Loss: 0.6931\n",
      "\n",
      "Epoch [6/100], Batch [100/706], Loss: 0.6930\n",
      "\n",
      "Epoch [6/100], Batch [105/706], Loss: 0.6926\n",
      "\n",
      "Epoch [6/100], Batch [110/706], Loss: 0.6926\n",
      "\n",
      "Epoch [6/100], Batch [115/706], Loss: 0.6926\n",
      "\n",
      "Epoch [6/100], Batch [120/706], Loss: 0.6928\n",
      "\n",
      "Epoch [6/100], Batch [125/706], Loss: 0.6929\n",
      "\n",
      "Epoch [6/100], Batch [130/706], Loss: 0.6931\n",
      "\n",
      "Epoch [6/100], Batch [135/706], Loss: 0.6933\n",
      "\n",
      "Epoch [6/100], Batch [140/706], Loss: 0.6930\n",
      "\n",
      "Epoch [6/100], Batch [145/706], Loss: 0.6927\n",
      "\n",
      "Epoch [6/100], Batch [150/706], Loss: 0.6923\n",
      "\n",
      "Epoch [6/100], Batch [155/706], Loss: 0.6925\n",
      "\n",
      "Epoch [6/100], Batch [160/706], Loss: 0.6926\n",
      "\n",
      "Epoch [6/100], Batch [165/706], Loss: 0.6930\n",
      "\n",
      "Epoch [6/100], Batch [170/706], Loss: 0.6928\n",
      "\n",
      "Epoch [6/100], Batch [175/706], Loss: 0.6923\n",
      "\n",
      "Epoch [6/100], Batch [180/706], Loss: 0.6932\n",
      "\n",
      "Epoch [6/100], Batch [185/706], Loss: 0.6932\n",
      "\n",
      "Epoch [6/100], Batch [190/706], Loss: 0.6919\n",
      "\n",
      "Epoch [6/100], Batch [195/706], Loss: 0.6928\n",
      "\n",
      "Epoch [6/100], Batch [200/706], Loss: 0.6930\n",
      "\n",
      "Epoch [6/100], Batch [205/706], Loss: 0.6924\n",
      "\n",
      "Epoch [6/100], Batch [210/706], Loss: 0.6926\n",
      "\n",
      "Epoch [6/100], Batch [215/706], Loss: 0.6926\n",
      "\n",
      "Epoch [6/100], Batch [220/706], Loss: 0.6928\n",
      "\n",
      "Epoch [6/100], Batch [225/706], Loss: 0.6928\n",
      "\n",
      "Epoch [6/100], Batch [230/706], Loss: 0.6930\n",
      "\n",
      "Epoch [6/100], Batch [235/706], Loss: 0.6928\n",
      "\n",
      "Epoch [6/100], Batch [240/706], Loss: 0.6937\n",
      "\n",
      "Epoch [6/100], Batch [245/706], Loss: 0.6929\n",
      "\n",
      "Epoch [6/100], Batch [250/706], Loss: 0.6926\n",
      "\n",
      "Epoch [6/100], Batch [255/706], Loss: 0.6925\n",
      "\n",
      "Epoch [6/100], Batch [260/706], Loss: 0.6928\n",
      "\n",
      "Epoch [6/100], Batch [265/706], Loss: 0.6932\n",
      "\n",
      "Epoch [6/100], Batch [270/706], Loss: 0.6926\n",
      "\n",
      "Epoch [6/100], Batch [275/706], Loss: 0.6921\n",
      "\n",
      "Epoch [6/100], Batch [280/706], Loss: 0.6923\n",
      "\n",
      "Epoch [6/100], Batch [285/706], Loss: 0.6926\n",
      "\n",
      "Epoch [6/100], Batch [290/706], Loss: 0.6924\n",
      "\n",
      "Epoch [6/100], Batch [295/706], Loss: 0.6926\n",
      "\n",
      "Epoch [6/100], Batch [300/706], Loss: 0.6925\n",
      "\n",
      "Epoch [6/100], Batch [305/706], Loss: 0.6935\n",
      "\n",
      "Epoch [6/100], Batch [310/706], Loss: 0.6933\n",
      "\n",
      "Epoch [6/100], Batch [315/706], Loss: 0.6933\n",
      "\n",
      "Epoch [6/100], Batch [320/706], Loss: 0.6923\n",
      "\n",
      "Epoch [6/100], Batch [325/706], Loss: 0.6928\n",
      "\n",
      "Epoch [6/100], Batch [330/706], Loss: 0.6929\n",
      "\n",
      "Epoch [6/100], Batch [335/706], Loss: 0.6921\n",
      "\n",
      "Epoch [6/100], Batch [340/706], Loss: 0.6928\n",
      "\n",
      "Epoch [6/100], Batch [345/706], Loss: 0.6921\n",
      "\n",
      "Epoch [6/100], Batch [350/706], Loss: 0.6921\n",
      "\n",
      "Epoch [6/100], Batch [355/706], Loss: 0.6924\n",
      "\n",
      "Epoch [6/100], Batch [360/706], Loss: 0.6926\n",
      "\n",
      "Epoch [6/100], Batch [365/706], Loss: 0.6919\n",
      "\n",
      "Epoch [6/100], Batch [370/706], Loss: 0.6928\n",
      "\n",
      "Epoch [6/100], Batch [375/706], Loss: 0.6931\n",
      "\n",
      "Epoch [6/100], Batch [380/706], Loss: 0.6932\n",
      "\n",
      "Epoch [6/100], Batch [385/706], Loss: 0.6928\n",
      "\n",
      "Epoch [6/100], Batch [390/706], Loss: 0.6925\n",
      "\n",
      "Epoch [6/100], Batch [395/706], Loss: 0.6933\n",
      "\n",
      "Epoch [6/100], Batch [400/706], Loss: 0.6924\n",
      "\n",
      "Epoch [6/100], Batch [405/706], Loss: 0.6933\n",
      "\n",
      "Epoch [6/100], Batch [410/706], Loss: 0.6926\n",
      "\n",
      "Epoch [6/100], Batch [415/706], Loss: 0.6934\n",
      "\n",
      "Epoch [6/100], Batch [420/706], Loss: 0.6936\n",
      "\n",
      "Epoch [6/100], Batch [425/706], Loss: 0.6933\n",
      "\n",
      "Epoch [6/100], Batch [430/706], Loss: 0.6923\n",
      "\n",
      "Epoch [6/100], Batch [435/706], Loss: 0.6926\n",
      "\n",
      "Epoch [6/100], Batch [440/706], Loss: 0.6924\n",
      "\n",
      "Epoch [6/100], Batch [445/706], Loss: 0.6928\n",
      "\n",
      "Epoch [6/100], Batch [450/706], Loss: 0.6930\n",
      "\n",
      "Epoch [6/100], Batch [455/706], Loss: 0.6928\n",
      "\n",
      "Epoch [6/100], Batch [460/706], Loss: 0.6932\n",
      "\n",
      "Epoch [6/100], Batch [465/706], Loss: 0.6928\n",
      "\n",
      "Epoch [6/100], Batch [470/706], Loss: 0.6936\n",
      "\n",
      "Epoch [6/100], Batch [475/706], Loss: 0.6931\n",
      "\n",
      "Epoch [6/100], Batch [480/706], Loss: 0.6935\n",
      "\n",
      "Epoch [6/100], Batch [485/706], Loss: 0.6930\n",
      "\n",
      "Epoch [6/100], Batch [490/706], Loss: 0.6928\n",
      "\n",
      "Epoch [6/100], Batch [495/706], Loss: 0.6938\n",
      "\n",
      "Epoch [6/100], Batch [500/706], Loss: 0.6932\n",
      "\n",
      "Epoch [6/100], Batch [505/706], Loss: 0.6927\n",
      "\n",
      "Epoch [6/100], Batch [510/706], Loss: 0.6926\n",
      "\n",
      "Epoch [6/100], Batch [515/706], Loss: 0.6939\n",
      "\n",
      "Epoch [6/100], Batch [520/706], Loss: 0.6932\n",
      "\n",
      "Epoch [6/100], Batch [525/706], Loss: 0.6926\n",
      "\n",
      "Epoch [6/100], Batch [530/706], Loss: 0.6929\n",
      "\n",
      "Epoch [6/100], Batch [535/706], Loss: 0.6930\n",
      "\n",
      "Epoch [6/100], Batch [540/706], Loss: 0.6928\n",
      "\n",
      "Epoch [6/100], Batch [545/706], Loss: 0.6924\n",
      "\n",
      "Epoch [6/100], Batch [550/706], Loss: 0.6928\n",
      "\n",
      "Epoch [6/100], Batch [555/706], Loss: 0.6945\n",
      "\n",
      "Epoch [6/100], Batch [560/706], Loss: 0.6925\n",
      "\n",
      "Epoch [6/100], Batch [565/706], Loss: 0.6937\n",
      "\n",
      "Epoch [6/100], Batch [570/706], Loss: 0.6930\n",
      "\n",
      "Epoch [6/100], Batch [575/706], Loss: 0.6924\n",
      "\n",
      "Epoch [6/100], Batch [580/706], Loss: 0.6935\n",
      "\n",
      "Epoch [6/100], Batch [585/706], Loss: 0.6926\n",
      "\n",
      "Epoch [6/100], Batch [590/706], Loss: 0.6926\n",
      "\n",
      "Epoch [6/100], Batch [595/706], Loss: 0.6932\n",
      "\n",
      "Epoch [6/100], Batch [600/706], Loss: 0.6928\n",
      "\n",
      "Epoch [6/100], Batch [605/706], Loss: 0.6927\n",
      "\n",
      "Epoch [6/100], Batch [610/706], Loss: 0.6928\n",
      "\n",
      "Epoch [6/100], Batch [615/706], Loss: 0.6928\n",
      "\n",
      "Epoch [6/100], Batch [620/706], Loss: 0.6938\n",
      "\n",
      "Epoch [6/100], Batch [625/706], Loss: 0.6929\n",
      "\n",
      "Epoch [6/100], Batch [630/706], Loss: 0.6940\n",
      "\n",
      "Epoch [6/100], Batch [635/706], Loss: 0.6931\n",
      "\n",
      "Epoch [6/100], Batch [640/706], Loss: 0.6925\n",
      "\n",
      "Epoch [6/100], Batch [645/706], Loss: 0.6926\n",
      "\n",
      "Epoch [6/100], Batch [650/706], Loss: 0.6934\n",
      "\n",
      "Epoch [6/100], Batch [655/706], Loss: 0.6933\n",
      "\n",
      "Epoch [6/100], Batch [660/706], Loss: 0.6919\n",
      "\n",
      "Epoch [6/100], Batch [665/706], Loss: 0.6928\n",
      "\n",
      "Epoch [6/100], Batch [670/706], Loss: 0.6935\n",
      "\n",
      "Epoch [6/100], Batch [675/706], Loss: 0.6933\n",
      "\n",
      "Epoch [6/100], Batch [680/706], Loss: 0.6932\n",
      "\n",
      "Epoch [6/100], Batch [685/706], Loss: 0.6930\n",
      "\n",
      "Epoch [6/100], Batch [690/706], Loss: 0.6935\n",
      "\n",
      "Epoch [6/100], Batch [695/706], Loss: 0.6933\n",
      "\n",
      "Epoch [6/100], Batch [700/706], Loss: 0.6931\n",
      "\n",
      "Epoch [6/100], Batch [705/706], Loss: 0.6945\n",
      "\n",
      "Epoch [7/100], Batch [0/706], Loss: 0.6930\n",
      "\n",
      "Epoch [7/100], Batch [5/706], Loss: 0.6935\n",
      "\n",
      "Epoch [7/100], Batch [10/706], Loss: 0.6926\n",
      "\n",
      "Epoch [7/100], Batch [15/706], Loss: 0.6940\n",
      "\n",
      "Epoch [7/100], Batch [20/706], Loss: 0.6926\n",
      "\n",
      "Epoch [7/100], Batch [25/706], Loss: 0.6928\n",
      "\n",
      "Epoch [7/100], Batch [30/706], Loss: 0.6916\n",
      "\n",
      "Epoch [7/100], Batch [35/706], Loss: 0.6932\n",
      "\n",
      "Epoch [7/100], Batch [40/706], Loss: 0.6927\n",
      "\n",
      "Epoch [7/100], Batch [45/706], Loss: 0.6938\n",
      "\n",
      "Epoch [7/100], Batch [50/706], Loss: 0.6930\n",
      "\n",
      "Epoch [7/100], Batch [55/706], Loss: 0.6934\n",
      "\n",
      "Epoch [7/100], Batch [60/706], Loss: 0.6930\n",
      "\n",
      "Epoch [7/100], Batch [65/706], Loss: 0.6926\n",
      "\n",
      "Epoch [7/100], Batch [70/706], Loss: 0.6930\n",
      "\n",
      "Epoch [7/100], Batch [75/706], Loss: 0.6936\n",
      "\n",
      "Epoch [7/100], Batch [80/706], Loss: 0.6930\n",
      "\n",
      "Epoch [7/100], Batch [85/706], Loss: 0.6930\n",
      "\n",
      "Epoch [7/100], Batch [90/706], Loss: 0.6931\n",
      "\n",
      "Epoch [7/100], Batch [95/706], Loss: 0.6926\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Training loop\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mAlexnet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[26], line 15\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[0;32m     12\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, target)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Update the weights\u001b[39;00m\n\u001b[0;32m     18\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\aditya\\anaconda3\\envs\\lenet_5\\lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\aditya\\anaconda3\\envs\\lenet_5\\lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(Alexnet(num_classes=2).parameters(), lr=0.01)\n",
    "\n",
    "# Specify the number of training epochs\n",
    "num_epochs = 100\n",
    "\n",
    "# Training loop\n",
    "train(Alexnet(2), train_loader, criterion, optimizer, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lenet_5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
